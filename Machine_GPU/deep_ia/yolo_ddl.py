import os
import urllib.request
import subprocess
import torch
from ultralytics import YOLO

# üìÇ D√©finition des chemins des mod√®les
MODELS_DIR = os.path.expanduser("~/deep_ia/models")
YOLOV8_MODEL = os.path.join(MODELS_DIR, "yolov8n.pt")
YOLOV8_ONNX = os.path.join(MODELS_DIR, "yolov8n/1/model.onnx")
YOLOV8_CONFIG = os.path.join(MODELS_DIR, "yolov8n/config.pbtxt")

# üì• T√©l√©chargement de YOLOv8 si absent
YOLOV8_URL = "https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt"

def download_model(url, output_path):
    """T√©l√©charge un mod√®le si absent"""
    if not os.path.exists(output_path):
        print(f"üì• T√©l√©chargement de {output_path}...")
        urllib.request.urlretrieve(url, output_path)
        print(f"‚úÖ {output_path} t√©l√©charg√© avec succ√®s.")
    else:
        print(f"‚ÑπÔ∏è {output_path} d√©j√† pr√©sent.")

# üîΩ T√©l√©charger YOLOv8
download_model(YOLOV8_URL, YOLOV8_MODEL)

# üöÄ Conversion YOLOv8 en ONNX
def export_yolov8_to_onnx(pt_model, onnx_output):
    """Convertit un mod√®le YOLOv8 en ONNX"""
    print(f"üöÄ Exportation de {pt_model} en ONNX...")

    model = YOLO(pt_model)
    model.export(format="onnx", opset=9, dynamic=True)

    os.rename(pt_model.replace(".pt", ".onnx"), onnx_output)
    print(f"‚úÖ Mod√®le export√© en ONNX : {onnx_output}")

# üìÇ Cr√©er les dossiers pour Triton
os.makedirs(os.path.dirname(YOLOV8_ONNX), exist_ok=True)

# üõ† Exporter YOLOv8 en ONNX
export_yolov8_to_onnx(YOLOV8_MODEL, YOLOV8_ONNX)

# üõ† G√©n√©ration du fichier `config.pbtxt` pour YOLOv8
config_yolov8 = f"""
name: "yolov8n"
platform: "onnxruntime"
max_batch_size: 4
input [
  {{
    name: "images"
    data_type: TYPE_FP32
    dims: [-1, 3, 640, 640]
  }}
]
output [
  {{
    name: "output0"
    data_type: TYPE_FP32
    dims: [-1, 84, -1]
  }}
]
"""

# ‚úçÔ∏è √âcriture du fichier `config.pbtxt`
with open(YOLOV8_CONFIG, "w") as f:
    f.write(config_yolov8.strip())
print(f"‚úÖ Configuration Triton g√©n√©r√©e : {YOLOV8_CONFIG}")

# üöÄ D√©marrage de Triton avec YOLOv8
print("üöÄ D√©marrage de Triton Inference Server...")
subprocess.run([
    "sudo", "docker", "run", "--rm", "--gpus", "all",
    "-p", "8000:8000", "-p", "8001:8001", "-p", "8002:8002",
    "-v", f"{MODELS_DIR}:/models",
    "nvcr.io/nvidia/tritonserver:24.04-py3",
    "tritonserver", "--model-repository=/models"
])
